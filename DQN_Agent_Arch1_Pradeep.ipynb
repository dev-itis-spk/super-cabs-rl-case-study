{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8umnh26Rxvz"
   },
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1626579449989,
     "user": {
      "displayName": "Manoj Romina",
      "photoUrl": "",
      "userId": "05949028983322850580"
     },
     "user_tz": -330
    },
    "id": "z8xmw2p3Rxv2"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from collections import deque\n",
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "\n",
    "# for building DQN model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "# for plotting graphs\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22264,
     "status": "ok",
     "timestamp": 1626577662000,
     "user": {
      "displayName": "Manoj Romina",
      "photoUrl": "",
      "userId": "05949028983322850580"
     },
     "user_tz": -330
    },
    "id": "2svwSK0pSWkY",
    "outputId": "595eae5b-234d-4079-b0ba-aa98305c98c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# Mounting Google Drive\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 474,
     "status": "ok",
     "timestamp": 1626577666322,
     "user": {
      "displayName": "Manoj Romina",
      "photoUrl": "",
      "userId": "05949028983322850580"
     },
     "user_tz": -330
    },
    "id": "O8a7ErrUSjHE"
   },
   "outputs": [],
   "source": [
    "# Changing the location to the Data Folder\n",
    "import os\n",
    "os.chdir('/content/gdrive/MyDrive/Colab Notebooks')  #change dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 671,
     "status": "ok",
     "timestamp": 1626579456915,
     "user": {
      "displayName": "Manoj Romina",
      "photoUrl": "",
      "userId": "05949028983322850580"
     },
     "user_tz": -330
    },
    "id": "qoXeK-OlSv29"
   },
   "outputs": [],
   "source": [
    "# Import the environment\n",
    "from Env import CabDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1626579456916,
     "user": {
      "displayName": "Manoj Romina",
      "photoUrl": "",
      "userId": "05949028983322850580"
     },
     "user_tz": -330
    },
    "id": "8XL-C_eLRxv-"
   },
   "outputs": [],
   "source": [
    "# Suppressing warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtYN-Jk4Rxv_"
   },
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1626579457385,
     "user": {
      "displayName": "Manoj Romina",
      "photoUrl": "",
      "userId": "05949028983322850580"
     },
     "user_tz": -330
    },
    "id": "ooCgIBqyRxv_"
   },
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(\"TM.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1626579457982,
     "user": {
      "displayName": "Manoj Romina",
      "photoUrl": "",
      "userId": "05949028983322850580"
     },
     "user_tz": -330
    },
    "id": "e5lZUY0MRxwA",
    "outputId": "9834f164-0bc4-4546-b29c-186bec1a14ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:  <class 'numpy.ndarray'>\n",
      "Max:  11.0\n",
      "Min:  0.0\n",
      "Mean:  3.0542857142857143\n",
      "Var:  7.93705306122449\n"
     ]
    }
   ],
   "source": [
    "# checking the contents of Time_matrix\n",
    "print(\"Type: \", type(Time_matrix))\n",
    "print(\"Max: \", Time_matrix.max())\n",
    "print(\"Min: \", Time_matrix.min())\n",
    "print(\"Mean: \", Time_matrix.mean())\n",
    "print(\"Var: \", Time_matrix.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrViHflFRxwD"
   },
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SuYCIPdPRxwD"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1626579460044,
     "user": {
      "displayName": "Manoj Romina",
      "photoUrl": "",
      "userId": "05949028983322850580"
     },
     "user_tz": -330
    },
    "id": "vM49dtQ3RxwF",
    "outputId": "6050fa77-56a9-4294-947a-65db95ff238a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "episodes = np.arange(0, 15_000)\n",
    "epsilon = []\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.00001\n",
    "\n",
    "for i in range(0, 15_000):\n",
    "    epsilon.append(min_epsilon + (max_epsilon - min_epsilon) * np.exp(-0.0009 * i))\n",
    "    \n",
    "plt.plot(episodes, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSygtQ4jRxwG"
   },
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1626579462591,
     "user": {
      "displayName": "Manoj Romina",
      "photoUrl": "",
      "userId": "05949028983322850580"
     },
     "user_tz": -330
    },
    "id": "5Am7c8DuRxwH"
   },
   "outputs": [],
   "source": [
    "# Invoke CabDriver Env class\n",
    "env = CabDriver()\n",
    "action_space, state_space, state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1626580499335,
     "user": {
      "displayName": "Manoj Romina",
      "photoUrl": "",
      "userId": "05949028983322850580"
     },
     "user_tz": -330
    },
    "id": "wxlKbM8uRxwH"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"Deep Q learning Agent class\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN\n",
    "        self.discount_factor = 0.95\n",
    "        self.learning_rate = 0.01\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_max = 1.0\n",
    "        self.epsilon_decay = -0.0009\n",
    "        self.epsilon_min = 0.00001\n",
    "        self.batch_size = 32\n",
    "        \n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        \n",
    "        # tracking state [0, 0, 0] and action (0, 2) at index 2 in the action space\n",
    "        self.track_state = np.array(env.state_encode_arch1([0, 0, 0])).reshape(1, 36)\n",
    "        \n",
    "        # initializing the state values tracked\n",
    "        self.states_tracked = []\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        \"\"\"Function to build a neural network\"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "        # output layer\n",
    "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        # model.summary\n",
    "        return model\n",
    "\n",
    "    def get_action(self, state, possible_actions_index, actions, episode):\n",
    "        \"\"\"Choosing an action for a given state and episode based on epsilon-greedy policy\"\"\"\n",
    "        # calculating decay factor for given episode\n",
    "        self.epsilon = self.epsilon_min + ((self.epsilon_max - self.epsilon_min) * np.exp(self.epsilon_decay * episode))\n",
    "\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # Exploration: choosing a random action from ride requests\n",
    "            return random.choice(possible_actions_index)\n",
    "        else:\n",
    "            # Exploitation: choose the action with the highest state action value\n",
    "            # Reshape state to (1, state_size), so that the first index corresponds to the batch size\n",
    "            encoded_states = np.array(env.state_encode_arch1(state)).reshape(1, self.state_size)\n",
    "\n",
    "            # predict q values for all possible actions of a given state\n",
    "            pred_q_values = self.model.predict(encoded_states)\n",
    "\n",
    "            # Filter/Truncate the array of predicted q values corresponding to selected requests\n",
    "            # for a given location i.e, to only those actions that are part of the ride requests.\n",
    "            possible_q_values = [pred_q_values[0][i] for i in possible_actions_index]\n",
    "\n",
    "            # choosing request with high q value and sending the correspoding action\n",
    "            return possible_actions_index[np.argmax(possible_q_values)]\n",
    "\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Appending state, action, reward, next_state to replay buffer\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "        # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"Train the neural network\"\"\"\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            # initializing 2 matrices: update_input and update_output\n",
    "            update_output = np.zeros((self.batch_size, self.state_size))\n",
    "            update_input = np.zeros((self.batch_size, self.state_size))\n",
    "            actions, rewards, done = [], [], []\n",
    "\n",
    "            for i in range(self.batch_size):\n",
    "                state, action, reward, next_state, is_done = mini_batch[i]\n",
    "                update_input[i] = env.state_encode_arch1(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                update_output[i] = env.state_encode_arch1(next_state)\n",
    "                done.append(is_done)\n",
    "\n",
    "            # predicting the target q values from state s\n",
    "            target = self.model.predict(update_input)\n",
    "            # target for q network\n",
    "            target_q_val = self.model.predict(update_output)\n",
    "\n",
    "            # update the target values\n",
    "            for i in range(self.batch_size):\n",
    "                if done[i]:  # if terminal state\n",
    "                    target[i][actions[i]] = rewards[i]\n",
    "                else:  # if non terminal state\n",
    "                    target[i][actions[i]] = rewards[i] + (self.discount_factor * np.max(target_q_val[i]))\n",
    "\n",
    "            # fitting model and tracking the loss values\n",
    "            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "\n",
    "    def save_tracking_states(self):\n",
    "        # using the model to predict q value of the state we are tracking\n",
    "        q_value = self.model.predict(self.track_state)\n",
    "        # grabbing the q value of action index we are tracking\n",
    "        self.states_tracked.append(q_value[0][2])\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(\"model.h5\")\n",
    "        with open(name, 'wb') as file:\n",
    "            pickle.dump(self.model.get_weights(), file, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxYVg6mPRxwJ"
   },
   "source": [
    "### DQN block\n",
    "for episode in range(EPISODES):\n",
    "\n",
    "    - # Write code here\n",
    "    - # Call the environment\n",
    "    - # Call all the initialised variables of the environment\n",
    "    \n",
    "\n",
    "    - #Call the DQN agent\n",
    "    \n",
    "    \n",
    "    - while !terminal_state:\n",
    "        \n",
    "        -- # Write your code here\n",
    "        -- # 1. Pick epsilon-greedy action from possible actions for the current state\n",
    "        -- # 2. Evaluate your reward and next state\n",
    "        -- # 3. Append the experience to the memory\n",
    "        -- # 4. Train the model by calling function agent.train_model\n",
    "        -- # 5. Keep a track of rewards, Q-values, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1626580512821,
     "user": {
      "displayName": "Manoj Romina",
      "photoUrl": "",
      "userId": "05949028983322850580"
     },
     "user_tz": -330
    },
    "id": "4DT6F9_XRxwJ"
   },
   "outputs": [],
   "source": [
    "# episodes and other parameters\n",
    "episode_time = 24 * 30  # 30 days before which car has to be recharged\n",
    "EPISODES = 500\n",
    "m = 5\n",
    "t = 24\n",
    "d = 7\n",
    "\n",
    "# Set up state and action sizes.\n",
    "state_size = m + t + d\n",
    "action_size = len(action_space)\n",
    "\n",
    "# Invoke agent class\n",
    "agent = DQNAgent(action_size=action_size, state_size=state_size)\n",
    "\n",
    "# to store rewards in each episode\n",
    "rewards_per_episode, episodes = [], []\n",
    "\n",
    "# Rewards for state [0, 0, 0] being tracked.\n",
    "rewards_init_state = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0dpTR5ARxwK"
   },
   "source": [
    "##### Run the episodes, build up replay buffer and train the model.\n",
    "Note:\n",
    "The moment the total episode time exceeds 720 (30 days), we ignore the most recent ride and do NOT save that experience in the replay memory. <br>\n",
    "The init state is randomly picked from the state space for each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LXdD9YTXRxwK",
    "outputId": "b0938b9f-3813-4cec-8b4b-0c1b97daa6a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating Episodic Training\n",
      "Episode: 99, Reward: -148.0, Memory Length: 2000, Agent Epsilon: 0.9147549460502341, Total Time: 726.0\n",
      "Saving model at 99\n",
      "Episode: 199, Reward: -105.0, Memory Length: 2000, Agent Epsilon: 0.8360239327645561, Total Time: 723.0\n",
      "Saving model at 199\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "score_tracked = []\n",
    "print(\"Initiating Episodic Training\")\n",
    "for episode in range(EPISODES):\n",
    "    terminal_state = False\n",
    "    score = 0\n",
    "    track_reward = False\n",
    "    \n",
    "    # recreating new object at each episode i.e, resetting\n",
    "    env = CabDriver()\n",
    "    action_space, state_space, state = env.reset()\n",
    "    \n",
    "    # saving the initial state so that reward can be tracked if state is [0, 0, 0]\n",
    "    initial_state = env.state_init\n",
    "    \n",
    "    total_time = 0\n",
    "    while not terminal_state:\n",
    "        # getting a list of ride requests the driver got\n",
    "        possible_actions_indices, actions = env.get_requests(state)\n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state\n",
    "        action = agent.get_action(state, possible_actions_indices, actions, episode)\n",
    "        # 2. Evaluate your reward and next state\n",
    "        reward, next_state, ride_time = env.step(state, env.action_space[action], Time_matrix)\n",
    "        \n",
    "        # total time taken for the ride\n",
    "        total_time += ride_time\n",
    "        \n",
    "        if total_time > episode_time:\n",
    "            # if the total time taken crosses the time limit of 720, stop the episode\n",
    "            terminal_state = True\n",
    "        else:\n",
    "            # 3. Append the experience to the memory\n",
    "            agent.append_sample(state, action, reward, next_state, terminal_state)\n",
    "            \n",
    "            if ((episode + 1)) % 20 == 0:\n",
    "              # 4. Train the model by calling function agent.train_model\n",
    "              agent.train_model()\n",
    "\n",
    "            # 5. Keep a track of rewards, Q-values, loss\n",
    "            score += reward\n",
    "            state = next_state\n",
    "            \n",
    "    # store total reward obtained in this episode\n",
    "    rewards_per_episode.append(score)\n",
    "    episodes.append(episode)\n",
    "    \n",
    "    # print status for every 100 episodes\n",
    "    if ((episode + 1) % 100) == 0:\n",
    "        print(f\"Episode: {episode}, Reward: {score}, Memory Length: {len(agent.memory)}\" + \n",
    "              f\", Agent Epsilon: {agent.epsilon}, Total Time: {total_time}\")\n",
    "        \n",
    "    # saving the q value of the state-action pair we are tracking after every 50\n",
    "    if (episode + 1) % 50 == 0:\n",
    "        agent.save_tracking_states()\n",
    "    \n",
    "    # total rewards per episode\n",
    "    score_tracked.append(score)\n",
    "    \n",
    "    # save model for every 1000 episode\n",
    "    if (episode + 1) % 100 == 0 and episode != 0:\n",
    "        print(f\"Saving model at {episode}\")\n",
    "        agent.save(name=\"model_weights.pkl\")\n",
    "        \n",
    "print(f\"Total Elapsed Time: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kd9451_QRxwL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcQIDz8ARxwM"
   },
   "source": [
    "### Tracking Convergence\n",
    "#### Tracking the state-action pairs for checking convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6L6YImsqRxwM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bq5BKBm7RxwM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36VwhxgnRxwM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWleZcBARxwM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DQN_Agent_Arch1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
